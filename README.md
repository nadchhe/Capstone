# Capstone
I evaluated two LLMs (Gemini and LLAMA) on a dataset to predict if the question asked is harmful/non-harmful 
This project involves using a dataset from the LLM-EvaluationHub to evaluate and compare the effectiveness of different language models in detecting harmful prompts. Participants are expected to implement at least two models, perform data visualization, and analyze which model best identifies harmful content.
